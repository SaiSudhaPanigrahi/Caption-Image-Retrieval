{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import dill\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.load_session('nearest_neighbor.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "n_train = 10000\n",
    "n_test = 2000\n",
    "top_k = 10\n",
    "n_predict = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features\n",
    "features_train_ff = pd.read_csv('data/features_train/features_resnet1000_train.csv', delimiter=',', index_col=0, header=None)\n",
    "features_test_ff = pd.read_csv('data/features_test/features_resnet1000_test.csv', delimiter=',', index_col=0, header=None)\n",
    "\n",
    "features_train_ff.index = features_train_ff.index.str.lstrip('images_train/').str.rstrip('.jpg')\n",
    "features_train_ff.index = pd.to_numeric(features_train_ff.index, errors='coerce')\n",
    "features_train_ff.sort_index(inplace=True)\n",
    "\n",
    "features_test_ff.index = features_test_ff.index.str.lstrip('images_test/').str.rstrip('.jpg')\n",
    "features_test_ff.index = pd.to_numeric(features_test_ff.index, errors='coerce')\n",
    "features_test_ff.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word preprocessing\n",
    "import re\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class Preprocess(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        # self.stopList = set(stopwords.words(\"english\"))\n",
    "        self.stopList = set([word.encode('ascii', 'ignore') for word in stopwords.words('english')])\n",
    "        \n",
    "    def preprocess(self, string, n_gram=1):\n",
    "        \n",
    "        # replace special character with space\n",
    "        string = re.sub(r'[^a-zA-Z0-9 ]', r' ', string).encode('ascii', 'ignore')\n",
    "        \n",
    "        # Lemmatization (handles capitalization), ignoring stop word\n",
    "        # turn output to ASCII and ignore special character\n",
    "        ans = [self.stemmer.stem(word).encode('ascii', 'ignore') for word in string.split()]\n",
    "        ans = [word for word in ans if word not in self.stopList]\n",
    "        \n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load descriptions\n",
    "descriptions_train = [set()] * n_train\n",
    "processor = Preprocess()\n",
    "for i in range(n_train):\n",
    "    with open('data/descriptions_train/' + str(i) + '.txt') as f:\n",
    "        words = f.read() # readlines()\n",
    "        descriptions_train[i] = processor.preprocess(words)\n",
    "\n",
    "descriptions_test = [set()] * n_test\n",
    "for i in range(n_test):\n",
    "    with open('data/descriptions_test/' + str(i) + '.txt') as f:\n",
    "        words = f.read() # readlines()\n",
    "        descriptions_test[i] = processor.preprocess(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6409\n",
      "('train/test: ', 0)\n",
      "('train/test: ', 1)\n"
     ]
    }
   ],
   "source": [
    "# get bag of words features\n",
    "def BagofWords(train, test):\n",
    "    bag = set()\n",
    "    for words in train:\n",
    "        bag |= set(words)\n",
    "    bag = list(bag)\n",
    "    bag_idx = {x:i for i, x in enumerate(bag)}\n",
    "    print(len(bag))\n",
    "    # print(bag)\n",
    "    \n",
    "    # create feature vectors\n",
    "    train_features = np.zeros((len(train), len(bag)))\n",
    "    test_features = np.zeros((len(test), len(bag)))\n",
    "\n",
    "    data = [train, test]\n",
    "    features = [train_features, test_features]\n",
    "    # '''\n",
    "    for k in [0,1]:\n",
    "        print('train/test: ', k)\n",
    "        for i in xrange(len(data[k])):\n",
    "            # if i%500 == 0: print(k, i)\n",
    "            for word in data[k][i]:\n",
    "                try:\n",
    "                    features[k][i, bag_idx[word] ] += 1\n",
    "                except KeyError: pass\n",
    "    # '''\n",
    "    return train_features, test_features, bag, bag_idx\n",
    "\n",
    "train_features, test_features, bag, bag_idx = BagofWords(descriptions_train, descriptions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-process: L2 normalization\n",
    "from sklearn.preprocessing import normalize\n",
    "train_features = normalize(train_features, norm='l2', axis=1)\n",
    "test_features = normalize(test_features, norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive Examples\n",
    "pos = [0 for i in range(10000)]\n",
    "for i in range(10000):\n",
    "    pos[i] = np.concatenate((features_train_ff.values[i],train_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative Examples\n",
    "neg = [0 for i in range(10000)]\n",
    "for i in range(10000):\n",
    "    j = random.randint(0,10000-1)\n",
    "    while j == i:\n",
    "        j = random.randint(0,10000-1)\n",
    "    neg[i] = np.concatenate((features_train_ff.values[i],train_features[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "#Lables for Neural Network\n",
    "y_1 = np.ones(10000)\n",
    "y_0 = np.zeros(10000)\n",
    "y = np.concatenate((y_1,y_0))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X data for Neural Network\n",
    "X = np.concatenate((pos,neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='adam', hidden_layer_sizes=(1000,100), max_iter = 100, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "#3:55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = clf.predict_proba([np.concatenate((features_train_ff.values[0], train_features[10])) ] )\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run me\n",
    "#ans  = [[0 for i in range(2000)] for j in range(2000)]\n",
    "#for i in range(2000):\n",
    "    #for j in range(2000):\n",
    "        # ans[i][j] = clf.predict_proba([np.concatenate((features_test_ff.values[i], test_features[j])) ] )[0, 1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_vector = []\n",
    "#for i in range(2000):\n",
    "    #for j in range(2000):\n",
    "        #test_vector.append(np.concatenate((features_test_ff.values[i], test_features[j])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_1 = []\n",
    "for i in range(2000):\n",
    "    test_vector = []\n",
    "    for j in range(2000):\n",
    "        test_vector.append(np.concatenate((features_test_ff.values[i], test_features[j])))\n",
    "    ans_1.append( clf.predict_proba(test_vector) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = np.asarray(ans_1)\n",
    "ans = ans[:,:,1]\n",
    "ans = np.transpose(ans)\n",
    "print(ans.shape)\n",
    "ans_sort = np.sort(ans, axis=1)[:, ::-1]\n",
    "predict_neural = np.argsort(ans, axis=1)[:, ::-1]\n",
    "print(predict_neural.shape)\n",
    "predict_neural = predict_neural[:, :n_predict]\n",
    "predict_neural.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(predict, label):\n",
    "    print(len(predict), len(label))\n",
    "    assert(len(predict) == len(label))\n",
    "    score = 0\n",
    "    for i in range(len(predict)):\n",
    "        try:\n",
    "            idx = predict[i].index(label[i])\n",
    "            score += (21 - idx) / 20\n",
    "        except ValueError:\n",
    "            print(label[i], predict[i])\n",
    "            pass\n",
    "    score /= len(predict)\n",
    "    print(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert prediction to '0.jpg'\n",
    "test_predict_str = [None] * n_test\n",
    "for i in range(n_test):\n",
    "    res = ' '.join([str(int(x)) + '.jpg' for x in predict_neural[i]])\n",
    "    test_predict_str[i] = res # ' '.join([str(int(x)) + '.jpg' for x in test_predict[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "df = pd.DataFrame(data=test_predict_str)\n",
    "df.index = [str(x) + '.txt' for x in range(n_test)]\n",
    "df.to_csv('./neural_net.csv', mode='w', index=True, index_label='Descritpion_ID', header=['Top_20_Image_IDs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
