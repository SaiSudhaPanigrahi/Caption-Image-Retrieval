{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import dill\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.load_session('nearest_neighbor.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word vectors successfully!\n"
     ]
    }
   ],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "print(\"Loaded word vectors successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "num_train = 8000\n",
    "num_dev = 2000\n",
    "num_test = 2000\n",
    "num_predict = 20\n",
    "\n",
    "split_idx = list(range(num_train + num_dev))\n",
    "random.shuffle(split_idx)\n",
    "\n",
    "top_k_d = 10\n",
    "top_k_i = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all x matrices!\n",
      "x_train shape: (8000, 300)\n",
      "x_dev shape: (2000, 300)\n",
      "x_test shape: (2000, 300)\n"
     ]
    }
   ],
   "source": [
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path) as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n",
    "\n",
    "def doc_to_vec(sentence, word2vec):\n",
    "    # get list of word vectors in sentence\n",
    "    word_vecs = [word2vec.get_vector(w) for w in sentence.split() if w in word2vec.vocab]\n",
    "    # return average\n",
    "    return np.stack(word_vecs).mean(0)\n",
    "\n",
    "# build x matrices\n",
    "train_dev_desc = parse_descriptions(\"data/descriptions_train\", num_doc=(num_train+num_dev))\n",
    "test_desc = parse_descriptions(\"data/descriptions_test\", num_doc=num_test)\n",
    "d_train = np.array([doc_to_vec(train_dev_desc[i], word2vec) for i in split_idx[:num_train]])\n",
    "d_dev = np.array([doc_to_vec(train_dev_desc[i], word2vec) for i in split_idx[num_train:]])\n",
    "d_test = np.array([doc_to_vec(d, word2vec) for d in test_desc])\n",
    "\n",
    "print(\"Built all x matrices!\")\n",
    "print(\"x_train shape:\", d_train.shape)\n",
    "print(\"x_dev shape:\", d_dev.shape)\n",
    "print(\"x_test shape:\", d_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word preprocessing\n",
    "import re\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class Preprocess(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        # self.stopList = set(stopwords.words(\"english\"))\n",
    "        self.stopList = set([word.encode('ascii', 'ignore') for word in stopwords.words('english')])\n",
    "        \n",
    "    def preprocess(self, string, n_gram=1):\n",
    "        \n",
    "        # replace special character with space\n",
    "        string = re.sub(r'[^a-zA-Z0-9 ]', r' ', string).encode('ascii', 'ignore')\n",
    "        \n",
    "        # Lemmatization (handles capitalization), ignoring stop word\n",
    "        # turn output to ASCII and ignore special character\n",
    "        ans = [self.stemmer.stem(word).encode('ascii', 'ignore') for word in string.split()]\n",
    "        ans = [word for word in ans if word not in self.stopList]\n",
    "        \n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load descriptions\n",
    "descriptions_train = [set()] * n_train\n",
    "processor = Preprocess()\n",
    "for i in range(n_train):\n",
    "    with open('data/descriptions_train/' + str(i) + '.txt') as f:\n",
    "        words = f.read() # readlines()\n",
    "        descriptions_train[i] = processor.preprocess(words)\n",
    "\n",
    "descriptions_test = [set()] * n_test\n",
    "for i in range(n_test):\n",
    "    with open('data/descriptions_test/' + str(i) + '.txt') as f:\n",
    "        words = f.read() # readlines()\n",
    "        descriptions_test[i] = processor.preprocess(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6409\n",
      "('train/test: ', 0)\n",
      "('train/test: ', 1)\n"
     ]
    }
   ],
   "source": [
    "# get bag of words features\n",
    "def BagofWords(train, test):\n",
    "    bag = set()\n",
    "    for words in train:\n",
    "        bag |= set(words)\n",
    "    bag = list(bag)\n",
    "    bag_idx = {x:i for i, x in enumerate(bag)}\n",
    "    print(len(bag))\n",
    "    # print(bag)\n",
    "    \n",
    "    # create feature vectors\n",
    "    train_features = np.zeros((len(train), len(bag)))\n",
    "    test_features = np.zeros((len(test), len(bag)))\n",
    "\n",
    "    data = [train, test]\n",
    "    features = [train_features, test_features]\n",
    "    # '''\n",
    "    for k in [0,1]:\n",
    "        print('train/test: ', k)\n",
    "        for i in xrange(len(data[k])):\n",
    "            # if i%500 == 0: print(k, i)\n",
    "            for word in data[k][i]:\n",
    "                try:\n",
    "                    features[k][i, bag_idx[word] ] += 1\n",
    "                except KeyError: pass\n",
    "    # '''\n",
    "    return train_features, test_features, bag, bag_idx\n",
    "\n",
    "train_features, test_features, bag, bag_idx = BagofWords(descriptions_train, descriptions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-process: L2 normalization\n",
    "from sklearn.preprocessing import normalize\n",
    "train_features = normalize(train_features, norm='l2', axis=1)\n",
    "test_features = normalize(test_features, norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse ResNet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all y matrices!\n",
      "y_train shape: (8000, 1000)\n",
      "y_dev shape: (2000, 1000)\n",
      "y_test shape: (2000, 1000)\n"
     ]
    }
   ],
   "source": [
    "def parse_features(features_path):\n",
    "    vec_map = {}\n",
    "    with open(features_path) as f:\n",
    "        for row in csv.reader(f):\n",
    "            img_id = int(row[0].split(\"/\")[1].split(\".\")[0])\n",
    "            vec_map[img_id] = np.array([float(x) for x in row[1:]])\n",
    "    return np.array([v for k, v in sorted(vec_map.items())])\n",
    "\n",
    "i_train_dev = parse_features(\"data/features_train/features_resnet1000_train.csv\")\n",
    "i_train = i_train_dev[split_idx[:num_train]]\n",
    "i_dev = i_train_dev[split_idx[num_train:]]\n",
    "i_test = parse_features(\"data/features_test/features_resnet1000_test.csv\") # @ is matrix multiplication for Python 3\n",
    "\n",
    "print(\"Built all y matrices!\")\n",
    "print(\"y_train shape:\", i_train.shape)\n",
    "print(\"y_dev shape:\", i_dev.shape)\n",
    "print(\"y_test shape:\", i_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features\n",
    "features_train_ff = pd.read_csv('data/features_train/features_resnet1000_train.csv', delimiter=',', index_col=0, header=None)\n",
    "features_test_ff = pd.read_csv('data/features_test/features_resnet1000_test.csv', delimiter=',', index_col=0, header=None)\n",
    "\n",
    "features_train_ff.index = features_train_ff.index.str.lstrip('images_train/').str.rstrip('.jpg')\n",
    "features_train_ff.index = pd.to_numeric(features_train_ff.index, errors='coerce')\n",
    "features_train_ff.sort_index(inplace=True)\n",
    "\n",
    "features_test_ff.index = features_test_ff.index.str.lstrip('images_test/').str.rstrip('.jpg')\n",
    "features_test_ff.index = pd.to_numeric(features_test_ff.index, errors='coerce')\n",
    "features_test_ff.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN_prediction(train_index_img, test_index_img, train_index_caption, test_index_caption, dist_caption, dist_image, w_caption=2933, w_img=1):\n",
    "    assert(len(features_train_ff) == len(train_features))\n",
    "    assert(len(features_test_ff) == len(test_features))\n",
    "    \n",
    "    n_train = len(train_index_img)\n",
    "    n_test = len(test_index_img)\n",
    "    \n",
    "    # find closest caption in train \n",
    "    # dist_caption = cdist(test_features, train_features, metric='sqeuclidean')\n",
    "    closest_caption_idx = np.argpartition(dist_caption, top_k, axis=1)[:, :top_k]\n",
    "    closest_caption_dist = np.asarray([dist_caption[i, closest_caption_idx[i]] for i in range(len(dist_caption))])\n",
    "    \n",
    "    # find closest image in test\n",
    "    # dist_image = cdist(features_train_ff, features_test_ff, metric='sqeuclidean')\n",
    "    closest_image_idx = np.argpartition(dist_image, top_k, axis=1)[:, :top_k]\n",
    "    closest_image_dist = np.asarray([dist_image[i, closest_image_idx[i]] for i in range(len(dist_image))])\n",
    "    \n",
    "    # get 400 distances (dist_caption+dist_image) for each caption\n",
    "    dist_final = np.empty((n_test, top_k*top_k))\n",
    "    idx_final = np.empty((n_test, top_k*top_k))\n",
    "    for i in range(n_test):\n",
    "        for j in range(top_k):\n",
    "            d_caption = closest_caption_dist[i, j]\n",
    "            for k in range(top_k):\n",
    "                d_img = closest_image_dist[closest_caption_idx[i, j], k]\n",
    "                dist_final[i, j*top_k+k] = d_caption * w_caption + d_img * w_img\n",
    "                idx_final[i, j*top_k+k] = closest_image_idx[closest_caption_idx[i, j], k] # need to fix\n",
    "    \n",
    "    # reassign the labels\n",
    "    # print(test_index_img[3], inx_fina[i, j])\n",
    "    '''\n",
    "    for i in range(len(idx_final)):\n",
    "        for j in range(len(idx_final[0])):\n",
    "            idx_final[i, j] = test_index_img[int(idx_final[i, j])]\n",
    "    '''\n",
    "\n",
    "    # predict\n",
    "    dist_final_arg = np.argsort(dist_final, axis=1)\n",
    "    predict = [[] for _ in range(n_test)]\n",
    "    for i in range(n_test):\n",
    "        for j in range(top_k*top_k):\n",
    "            if len(predict[i]) != n_predict and idx_final[i, dist_final_arg[i, j]] not in predict[i]:\n",
    "                # print(idx_final[i, dist_final_arg[i, j]])\n",
    "                predict[i].append(idx_final[i, dist_final_arg[i, j]])\n",
    "                \n",
    "    for i in range(len(predict)):\n",
    "        for j in range(len(predict[0])):\n",
    "            predict[i][j] = test_index_img[int(predict[i][j])]\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(predict, label):\n",
    "    print(len(predict), len(label))\n",
    "    assert(len(predict) == len(label))\n",
    "    score = 0\n",
    "    for i in range(len(predict)):\n",
    "        try:\n",
    "            idx = predict[i].index(label[i])\n",
    "            score += (21 - idx) / 20\n",
    "        except ValueError:\n",
    "            # print(label[i], predict[i])\n",
    "            pass\n",
    "    score /= len(predict)\n",
    "    print(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precomputation\n",
    "from sklearn.model_selection import KFold\n",
    "n_splits = 3\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "train_index_img = [train_index for train_index, test_index in kf.split(features_train_ff)]\n",
    "test_index_img = [test_index for train_index, test_index in kf.split(features_train_ff)]\n",
    "train_index_caption = [train_index for train_index, test_index in kf.split(train_features)]\n",
    "test_index_caption = [test_index for train_index, test_index in kf.split(train_features)]\n",
    "\n",
    "# precompute the distances\n",
    "dist_caption, dist_image = [], []\n",
    "for i in range(n_splits):\n",
    "    dist_caption.append( cdist(train_features[test_index_caption[i]], train_features[train_index_caption[i]], metric='sqeuclidean') )\n",
    "    dist_image.append( cdist(features_train_ff.values[train_index_img[i]], features_train_ff.values[test_index_img[i]], metric='sqeuclidean') )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3334, 6666)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_caption[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-95a2981a7bd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# do 3 fold cross validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mw_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m# , 2000, 2500, 3000, 3500] # modify this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_captions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_caption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_captions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# do 3 fold cross validation\n",
    "w_captions = [1500]# , 2000, 2500, 3000, 3500] # modify this line\n",
    "accuracy = np.empty(len(w_captions))\n",
    "\n",
    "for i, w_caption in enumerate(w_captions):\n",
    "    for j in range(1):\n",
    "        cv_predict = kNN_prediction(train_index_img[j],\n",
    "                                    test_index_img[j],\n",
    "                                    train_index_caption[j],\n",
    "                                    test_index_caption[j],\n",
    "                                    dist_caption[j],\n",
    "                                    dist_image[j],\n",
    "                                    w_caption=w_caption)\n",
    "        \n",
    "        accuracy[i] += scoring(cv_predict, test_index_caption[j])\n",
    "accuracy /= n_splits\n",
    "plt.plot(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find closest caption in train\n",
    "dist_caption = cdist(test_features, train_features, metric='sqeuclidean')\n",
    "closest_caption_idx = np.argpartition(dist_caption, top_k, axis=1)[:, :top_k]\n",
    "closest_caption_dist = np.asarray([dist_caption[i, closest_caption_idx[i]] for i in range(len(dist_caption))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find closest image in test\n",
    "dist_image = cdist(features_train_ff.values, features_test_ff.values, metric='sqeuclidean')\n",
    "closest_image_idx = np.argpartition(dist_image, top_k, axis=1)[:, :top_k]\n",
    "closest_image_dist = np.asarray([dist_image[i, closest_image_idx[i]] for i in range(len(dist_image))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 400 distances (dist_caption+dist_image) for each caption\n",
    "dist_final = np.empty((n_test, top_k*top_k))\n",
    "idx_final = np.empty((n_test, top_k*top_k))\n",
    "w_caption = 2933 # caption_dist * weight + image_dist\n",
    "w_img = 1\n",
    "for i in range(n_test):\n",
    "    for j in range(top_k):\n",
    "        d_caption = closest_caption_dist[i, j]\n",
    "        for k in range(top_k):\n",
    "            d_img = closest_image_dist[closest_caption_idx[i, j], k]\n",
    "            dist_final[i, j*top_k+k] = d_caption * w_caption + d_img * w_img\n",
    "            idx_final[i, j*top_k+k] = closest_image_idx[closest_caption_idx[i, j], k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "dist_final_arg = np.argsort(dist_final, axis=1)\n",
    "predict = [[] for _ in range(n_test)]\n",
    "for i in range(n_test):\n",
    "    for j in range(top_k*top_k):\n",
    "        if len(predict[i]) != n_predict and idx_final[i, dist_final_arg[i, j]] not in predict[i]:\n",
    "            # print(idx_final[i, dist_final_arg[i, j]])\n",
    "            predict[i].append(idx_final[i, dist_final_arg[i, j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert prediction to '0.jpg'\n",
    "test_predict_str = [None] * n_test\n",
    "for i in range(n_test):\n",
    "    res = ' '.join([str(int(x)) + '.jpg' for x in predict[i]])\n",
    "    test_predict_str[i] = res # ' '.join([str(int(x)) + '.jpg' for x in test_predict[i]])\n",
    "    \n",
    "# write to csv\n",
    "df = pd.DataFrame(data=test_predict_str)\n",
    "df.index = [str(x) + '.txt' for x in range(n_test)]\n",
    "df.to_csv('./nearest_neighbor.csv', mode='w', index=True, index_label='Descritpion_ID', header=['Top_20_Image_IDs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "df = pd.DataFrame(data=test_predict_str)\n",
    "df.index = [str(x) + '.txt' for x in range(n_test)]\n",
    "df.to_csv('./nearest_neighbor.csv', mode='w', index=True, index_label='Descritpion_ID', header=['Top_20_Image_IDs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('nearest_neighbor.db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
