{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we load libraries, define our train/test split, and load the word2vec dictionary using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 8000\n",
    "num_dev = 2000\n",
    "num_test = 2000\n",
    "num_predict = 20\n",
    "split_idx = list(range(num_train + num_dev))\n",
    "random.shuffle(split_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word vectors successfully!\n"
     ]
    }
   ],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "print(\"Loaded word vectors successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Parse Descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all d matrices!\n",
      "d_train shape: (8000, 300)\n",
      "d_dev shape: (2000, 300)\n",
      "d_test shape: (2000, 300)\n"
     ]
    }
   ],
   "source": [
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path) as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n",
    "\n",
    "def doc_to_vec(sentence, word2vec):\n",
    "    # get list of word vectors in sentence\n",
    "    word_vecs = [word2vec.get_vector(w) for w in sentence.split() if w in word2vec.vocab]\n",
    "    # return average\n",
    "    return np.stack(word_vecs).mean(0)\n",
    "\n",
    "# build x matrices\n",
    "train_dev_desc = parse_descriptions(\"data/descriptions_train\", num_doc=(num_train+num_dev))\n",
    "test_desc = parse_descriptions(\"data/descriptions_test\", num_doc=num_test)\n",
    "d_train = np.array([doc_to_vec(train_dev_desc[i], word2vec) for i in split_idx[:num_train]])\n",
    "d_dev = np.array([doc_to_vec(train_dev_desc[i], word2vec) for i in split_idx[num_train:]])\n",
    "d_test = np.array([doc_to_vec(d, word2vec) for d in test_desc])\n",
    "\n",
    "print(\"Built all d matrices!\")\n",
    "print(\"d_train shape:\", d_train.shape)\n",
    "print(\"d_dev shape:\", d_dev.shape)\n",
    "print(\"d_test shape:\", d_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bag_of_words.BagofWords\n",
    "from bag_of_words import BagofWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all t matrices!\n",
      "t_train shape: (8000, 80)\n",
      "t_dev shape: (2000, 80)\n",
      "t_test shape: (2000, 80)\n"
     ]
    }
   ],
   "source": [
    "def parse_tags(data_dir, num_doc):\n",
    "    tags = []\n",
    "    for i in range(num_doc):\n",
    "        with open(data_dir + str(i) + '.txt') as f:\n",
    "            lines = f.readlines()\n",
    "            tag = []\n",
    "            for line in lines:\n",
    "                tag.append(line.split(':')[1].rstrip())\n",
    "            tags.append(tag)\n",
    "    return tags\n",
    "\n",
    "train_dev_tags = np.asarray(parse_tags('data/tags_train/', num_doc=(num_train+num_dev)))\n",
    "test_tags = np.asarray(parse_tags('data/tags_test/', num_doc=(num_test)))\n",
    "t_train = train_dev_tags[split_idx[:num_train]]\n",
    "t_dev = train_dev_tags[split_idx[num_train:]]\n",
    "t_test = test_tags\n",
    "\n",
    "bow = BagofWords(t_train)\n",
    "t_train = bow.getFeatures(t_train)\n",
    "t_dev = bow.getFeatures(t_dev)\n",
    "t_test = bow.getFeatures(t_test)\n",
    "\n",
    "print(\"Built all t matrices!\")\n",
    "print(\"t_train shape:\", t_train.shape)\n",
    "print(\"t_dev shape:\", t_dev.shape)\n",
    "print(\"t_test shape:\", t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Parse ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all y matrices!\n",
      "y_train shape: (8000, 1000)\n",
      "y_dev shape: (2000, 1000)\n",
      "y_test shape: (2000, 1000)\n"
     ]
    }
   ],
   "source": [
    "def parse_features(features_path):\n",
    "    vec_map = {}\n",
    "    with open(features_path) as f:\n",
    "        for row in csv.reader(f):\n",
    "            img_id = int(row[0].split(\"/\")[1].split(\".\")[0])\n",
    "            vec_map[img_id] = np.array([float(x) for x in row[1:]])\n",
    "    return np.array([v for k, v in sorted(vec_map.items())])\n",
    "\n",
    "i_train_dev = parse_features(\"data/features_train/features_resnet1000_train.csv\")\n",
    "i_train = i_train_dev[split_idx[:num_train]]\n",
    "i_dev = i_train_dev[split_idx[num_train:]]\n",
    "i_test = parse_features(\"data/features_test/features_resnet1000_test.csv\") # @ is matrix multiplication for Python 3\n",
    "\n",
    "print(\"Built all y matrices!\")\n",
    "print(\"y_train shape:\", i_train.shape)\n",
    "print(\"y_dev shape:\", i_dev.shape)\n",
    "print(\"y_test shape:\", i_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=300)\n",
    "pca.fit(i_train)\n",
    "i_train = pca.transform(i_train)\n",
    "i_dev = pca.transform(i_dev)\n",
    "i_test = pca.transform(i_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 680)\n"
     ]
    }
   ],
   "source": [
    "#Positive Examples\n",
    "x_train_pos = np.concatenate((d_train, i_train, t_train), axis=1)\n",
    "\n",
    "#Negative Examples\n",
    "x_train_neg = [0] * num_train\n",
    "for i in range(num_train):\n",
    "    j = random.randint(0,num_train-1)\n",
    "    while i == j:\n",
    "        j = random.randint(0,num_train-1)\n",
    "    x_train_neg[i] = np.concatenate((d_train[i], i_train[j], t_train[j]))\n",
    "x_train_neg = np.asarray(x_train_neg)\n",
    "\n",
    "# X data for Neural Network\n",
    "x_train = np.concatenate((x_train_pos, x_train_neg), axis=0)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000,)\n"
     ]
    }
   ],
   "source": [
    "#Lables for Neural Network\n",
    "y_train_pos = np.ones(num_train)\n",
    "y_train_neg = np.zeros(num_train)\n",
    "y_train = np.concatenate((y_train_pos, y_train_neg))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.20021577\n",
      "Iteration 2, loss = 0.69555866\n",
      "Iteration 3, loss = 0.69448791\n",
      "Iteration 4, loss = 0.69372181\n",
      "Iteration 5, loss = 0.69187643\n",
      "Iteration 6, loss = 0.68332317\n",
      "Iteration 7, loss = 0.66601162\n",
      "Iteration 8, loss = 0.62494358\n",
      "Iteration 9, loss = 0.56000786\n",
      "Iteration 10, loss = 0.50581570\n",
      "Iteration 11, loss = 0.46917107\n",
      "Iteration 12, loss = 0.44224239\n",
      "Iteration 13, loss = 0.41346863\n",
      "Iteration 14, loss = 0.37415275\n",
      "Iteration 15, loss = 0.34849208\n",
      "Iteration 16, loss = 0.32918319\n",
      "Iteration 17, loss = 0.30273855\n",
      "Iteration 18, loss = 0.28285980\n",
      "Iteration 19, loss = 0.26715746\n",
      "Iteration 20, loss = 0.23829419\n",
      "Iteration 21, loss = 0.23855665\n",
      "Iteration 22, loss = 0.22841834\n",
      "Iteration 23, loss = 0.21956649\n",
      "Iteration 24, loss = 0.20600087\n",
      "Iteration 25, loss = 0.20736671\n",
      "Iteration 26, loss = 0.20029406\n",
      "Iteration 27, loss = 0.18620050\n",
      "Iteration 28, loss = 0.17655849\n",
      "Iteration 29, loss = 0.18394252\n",
      "Iteration 30, loss = 0.17158515\n",
      "Iteration 31, loss = 0.16394822\n",
      "Iteration 32, loss = 0.16911973\n",
      "Iteration 33, loss = 0.15687842\n",
      "Iteration 34, loss = 0.16042560\n",
      "Iteration 35, loss = 0.16884252\n",
      "Iteration 36, loss = 0.15911496\n",
      "Iteration 37, loss = 0.14673366\n",
      "Iteration 38, loss = 0.14795824\n",
      "Iteration 39, loss = 0.14247554\n",
      "Iteration 40, loss = 0.13872309\n",
      "Iteration 41, loss = 0.13721959\n",
      "Iteration 42, loss = 0.15152064\n",
      "Iteration 43, loss = 0.14978957\n",
      "Iteration 44, loss = 0.15066937\n",
      "Iteration 45, loss = 0.14527093\n",
      "Iteration 46, loss = 0.13576172\n",
      "Iteration 47, loss = 0.13460592\n",
      "Iteration 48, loss = 0.12916785\n",
      "Iteration 49, loss = 0.13501036\n",
      "Iteration 50, loss = 0.12715032\n",
      "Iteration 51, loss = 0.13004812\n",
      "Iteration 52, loss = 0.13354347\n",
      "Iteration 53, loss = 0.12950670\n",
      "Iteration 54, loss = 0.13543083\n",
      "Iteration 55, loss = 0.12369518\n",
      "Iteration 56, loss = 0.12795493\n",
      "Iteration 57, loss = 0.13973421\n",
      "Iteration 58, loss = 0.12641104\n",
      "Iteration 59, loss = 0.13136156\n",
      "Iteration 60, loss = 0.12401910\n",
      "Iteration 61, loss = 0.12511151\n",
      "Iteration 62, loss = 0.13224154\n",
      "Iteration 63, loss = 0.12609593\n",
      "Iteration 64, loss = 0.13322811\n",
      "Iteration 65, loss = 0.12989641\n",
      "Iteration 66, loss = 0.11900967\n",
      "Iteration 67, loss = 0.12334752\n",
      "Iteration 68, loss = 0.13017052\n",
      "Iteration 69, loss = 0.12508284\n",
      "Iteration 70, loss = 0.12416914\n",
      "Iteration 71, loss = 0.12853405\n",
      "Iteration 72, loss = 0.12006494\n",
      "Iteration 73, loss = 0.11574747\n",
      "Iteration 74, loss = 0.11891097\n",
      "Iteration 75, loss = 0.12577895\n",
      "Iteration 76, loss = 0.11958189\n",
      "Iteration 77, loss = 0.11200763\n",
      "Iteration 78, loss = 0.10969438\n",
      "Iteration 79, loss = 0.12254708\n",
      "Iteration 80, loss = 0.11882464\n",
      "Iteration 81, loss = 0.11026731\n",
      "Iteration 82, loss = 0.11253882\n",
      "Iteration 83, loss = 0.12507581\n",
      "Iteration 84, loss = 0.11478156\n",
      "Iteration 85, loss = 0.11228275\n",
      "Iteration 86, loss = 0.10291911\n",
      "Iteration 87, loss = 0.11073215\n",
      "Iteration 88, loss = 0.10696415\n",
      "Iteration 89, loss = 0.11287626\n",
      "Iteration 90, loss = 0.11956303\n",
      "Iteration 91, loss = 0.11798358\n",
      "Iteration 92, loss = 0.11721751\n",
      "Iteration 93, loss = 0.12291826\n",
      "Iteration 94, loss = 0.11696570\n",
      "Iteration 95, loss = 0.11395456\n",
      "Iteration 96, loss = 0.12084678\n",
      "Iteration 97, loss = 0.13058972\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(680, 680, 680), learning_rate='adaptive',\n",
       "       learning_rate_init=0.01, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(680, 680, 680) , activation='relu', solver='adam', learning_rate='adaptive', learning_rate_init=0.01, \n",
    "                    max_iter=200, random_state=1, early_stopping=False, validation_fraction=0.1, warm_start=False, verbose=True)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13058971785945805"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf_1300_650 = clf\n",
    "# clf_1300_1300_650 = clf\n",
    "clf.n_iter_\n",
    "clf.n_layers_\n",
    "clf.loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25 50 75 100 125 150 175 200 225 250 275 300 325 350 375 400 425 450 475 500 525 550 575 600 625 650 675 700 725 750 775 800 825 850 875 900 925 950 975 1000 1025 1050 1075 1100 1125 1150 1175 1200 1225 1250 1275 1300 1325 1350 1375 1400 1425 1450 1475 1500 1525 1550 1575 1600 1625 1650 1675 1700 1725 1750 1775 1800 1825 1850 1875 1900 1925 1950 1975 "
     ]
    }
   ],
   "source": [
    "y_dev_score = [0] * num_dev\n",
    "for i in range(num_dev):\n",
    "    if i%25 == 0: print(i, end=' ')\n",
    "    d_dev_rep = np.repeat([d_dev[i]], num_dev, axis=0)\n",
    "    x_dev = np.concatenate((d_dev_rep, i_dev, t_dev), axis=1)\n",
    "    score = clf.predict_proba(x_dev)\n",
    "    y_dev_score[i] = np.transpose(score[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(2000, 300)\n",
      "(2000, 300)\n",
      "(2000, 80)\n",
      "(2000, 680)\n",
      "[[0.01421441 0.05601501 0.03154161 ... 0.         0.         0.        ]\n",
      " [0.01421441 0.05601501 0.03154161 ... 0.         0.         0.        ]\n",
      " [0.01421441 0.05601501 0.03154161 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.01421441 0.05601501 0.03154161 ... 0.         0.         0.        ]\n",
      " [0.01421441 0.05601501 0.03154161 ... 0.         0.         0.        ]\n",
      " [0.01421441 0.05601501 0.03154161 ... 0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-4.448195191532257"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "print(d_dev[i].shape)\n",
    "d_dev_rep = np.repeat([d_dev[i]], num_dev, axis=0)\n",
    "print(d_dev_rep.shape)\n",
    "print(i_dev.shape)\n",
    "print(t_dev.shape)\n",
    "x_dev = np.concatenate((d_dev_rep, i_dev, t_dev), axis=1)\n",
    "print(x_dev.shape)\n",
    "print(x_dev)\n",
    "np.sum(x_dev[0]-x_dev[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99999249e-01 7.50954586e-07]\n",
      " [3.30123125e-03 9.96698769e-01]\n",
      " [9.99999963e-01 3.69405185e-08]\n",
      " ...\n",
      " [9.99817515e-01 1.82485207e-04]\n",
      " [9.99999700e-01 3.00292245e-07]\n",
      " [9.99997055e-01 2.94453821e-06]]\n",
      "[[9.99407368e-01 5.92631848e-04]]\n"
     ]
    }
   ],
   "source": [
    "score = clf.predict_proba(x_dev)\n",
    "print(score)\n",
    "# score_single = clf.predict_proba(np.concatenate((d_dev[0], i_dev[1], t_dev[1])) )\n",
    "print( clf.predict_proba([np.concatenate((d_dev[500], i_dev[3], t_dev[1])) ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development MAP@20: 0.01\n",
      "Mean index of true image 164.6955\n",
      "Median index of true image 109.0\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "y_dev_score = np.asarray(y_dev_score)\n",
    "y_dev_predict = np.argsort(y_dev_score, axis=1)[:, ::-1]\n",
    "\n",
    "dev_scores = []\n",
    "dev_pos_list = []\n",
    "for i in range(num_dev):\n",
    "    dev_pos = np.where(y_dev_predict[i] == i)\n",
    "    dev_pos_list.append(dev_pos)\n",
    "    if dev_pos[0] < 20:\n",
    "        dev_scores.append((21 - dev_pos[0]) / 20)\n",
    "    else:\n",
    "        dev_scores.append(0.0)\n",
    "\n",
    "print(\"Development MAP@20:\", np.mean(dev_scores, dtype=np.float64))\n",
    "print(\"Mean index of true image\", np.mean(dev_pos_list))\n",
    "print(\"Median index of true image\", np.median(dev_pos_list))\n",
    "print(len(dev_pos_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 "
     ]
    }
   ],
   "source": [
    "y_test_score = [0] * num_test\n",
    "for i in range(num_test):\n",
    "    if i%10 == 0: print(i, end=' ')\n",
    "    d_test_rep = np.repeat([d_test[i]], num_test, axis=0)\n",
    "    x_test = np.concatenate((d_test_rep, i_test), axis=1)\n",
    "    score = clf.predict_proba(x_test)\n",
    "    y_test_score[i] = np.transpose(score[:, 1])\n",
    "    \n",
    "y_test_score = np.asarray(y_test_score)\n",
    "y_test_predict = np.argsort(y_test_score, axis=1)[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert prediction to '0.jpg'\n",
    "y_test_predict = y_test_predict[:, :20]\n",
    "test_predict_str = [None] * num_test\n",
    "for i in range(num_test):\n",
    "    res = ' '.join([str(int(x)) + '.jpg' for x in y_test_predict[i]])\n",
    "    test_predict_str[i] = res # ' '.join([str(int(x)) + '.jpg' for x in test_predict[i]])\n",
    "    \n",
    "# write to csv\n",
    "df = pd.DataFrame(data=test_predict_str)\n",
    "df.index = [str(x) + '.txt' for x in range(num_test)]\n",
    "df.to_csv('./neural_net.csv', mode='w', index=True, index_label='Descritpion_ID', header=['Top_20_Image_IDs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Next we test out our linear model on our development data, computing its MAP@20, and investigating the quality of the rankings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Development MAP@20:', 0.0)\n",
      "('Mean index of true image', nan)\n",
      "('Median index of true image', nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Keith/anaconda2/lib/python2.7/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/Keith/anaconda2/lib/python2.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "def dist_matrix(x1, x2):\n",
    "    return ((np.expand_dims(x1, 1) - np.expand_dims(x2, 0)) ** 2).sum(2) ** 0.5\n",
    "\n",
    "# test performance on development set\n",
    "y_dev_pred = reg.predict(d_dev)\n",
    "dev_distances = dist_matrix(y_dev_pred, y_dev)\n",
    "dev_scores = []\n",
    "dev_pos_list = []\n",
    "\n",
    "for i in range(num_dev):\n",
    "    pred_dist_idx = list(np.argsort(dev_distances[i]))\n",
    "    dev_pos = pred_dist_idx.index(i)\n",
    "    dev_pos = np.where(y_dev_pred[i] == i)\n",
    "    dev_pos_list.append(dev_pos)\n",
    "    if dev_pos < 20:\n",
    "        dev_scores.append(1 / (dev_pos + 1)) # think it's wrong\n",
    "    else:\n",
    "        dev_scores.append(0.0)\n",
    "\n",
    "print(\"Development MAP@20:\", np.mean(dev_scores))\n",
    "print(\"Mean index of true image\", np.mean(dev_pos_list))\n",
    "print(\"Median index of true image\", np.median(dev_pos_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "('Development MAP@20:', 0.042)\n",
    "('Mean index of true image', 114.067)\n",
    "('Median index of true image', 31.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dev_pred.shape\n",
    "dev_distances.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Finally we use our model to compute top-20 predictions on the test data that can be submitted to Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written!\n"
     ]
    }
   ],
   "source": [
    "# create test predictions\n",
    "x_train_all = np.concatenate([x_train, x_dev])\n",
    "y_train_all = np.concatenate([y_train, y_dev])\n",
    "reg_best.fit(x_train_all, y_train_all)\n",
    "y_test_pred = reg_best.predict(x_test)\n",
    "test_distances = dist_matrix(y_test_pred, y_test)\n",
    "pred_rows = []\n",
    "\n",
    "for i in range(num_test):\n",
    "    test_dist_idx = list(np.argsort(test_distances[i]))\n",
    "    top_20 = test_dist_idx[:20]\n",
    "    row = [\"%d.jpg\" % i for i in test_dist_idx[:20]]\n",
    "    pred_rows.append(\" \".join(row))\n",
    "\n",
    "with open(\"test_submission.csv\", \"w\") as f:\n",
    "    f.write(\"Descritpion_ID,Top_20_Image_IDs\\n\")\n",
    "    for i, row in enumerate(pred_rows):\n",
    "        f.write(\"%d.txt,%s\\n\" % (i, row))\n",
    "\n",
    "print(\"Output written!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
